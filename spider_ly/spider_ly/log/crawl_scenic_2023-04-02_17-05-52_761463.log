2023-04-02 17:05:52.881 | INFO     | scrapy.utils.log:log_scrapy_info:147 - Scrapy 2.6.3 started (bot: spider_ly)
2023-04-02 17:05:52.893 | INFO     | scrapy.utils.log:log_scrapy_info:154 - Versions: lxml 4.6.4.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.2.0, Python 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 22.0.0 (OpenSSL 1.1.1m  14 Dec 2021), cryptography 36.0.1, Platform Windows-10-10.0.19041-SP0
2023-04-02 17:05:52.898 | INFO     | scrapy.crawler:__init__:60 - Overridden settings:
{'AUTOTHROTTLE_ENABLED': True,
 'BOT_NAME': 'spider_ly',
 'CONCURRENT_REQUESTS_PER_DOMAIN': 24,
 'DUPEFILTER_CLASS': 'scrapy_redis.dupefilter.RFPDupeFilter',
 'LOG_ENABLED': False,
 'LOG_FILE': '../log/erro.log',
 'LOG_LEVEL': 'ERROR',
 'NEWSPIDER_MODULE': 'spider_ly.spiders',
 'RETRY_TIMES': 5,
 'SCHEDULER': 'scrapy_redis.scheduler.Scheduler',
 'SPIDER_MODULES': ['spider_ly.spiders']}
2023-04-02 17:05:52.931 | INFO     | scrapy.extensions.telnet:__init__:55 - Telnet Password: e140bd6b8e6f6667
2023-04-02 17:05:52.951 | INFO     | scrapy.middleware:from_settings:51 - Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.throttle.AutoThrottle']
2023-04-02 17:05:53.175 | INFO     | scrapy.middleware:from_settings:51 - Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'spider_ly.middlewares.ProxyMiddleWare',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2023-04-02 17:05:53.179 | INFO     | scrapy.middleware:from_settings:51 - Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2023-04-02 17:05:53.201 | INFO     | scrapy.middleware:from_settings:51 - Enabled item pipelines:
['spider_ly.pipelines.saveCsvScrapyPipeline',
 'spider_ly.pipelines.saveMysqlScrapyPipeline',
 'scrapy_redis.pipelines.RedisPipeline']
2023-04-02 17:05:53.201 | INFO     | scrapy.core.engine:open_spider:316 - Spider opened
2023-04-02 17:05:53.229 | INFO     | scrapy.extensions.logstats:log:48 - Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2023-04-02 17:05:53.231 | INFO     | scrapy.extensions.telnet:start_listening:67 - Telnet console listening on 127.0.0.1:6023
2023-04-02 17:05:55.445 | WARNING  | warnings:_showwarnmsg:109 - C:\Users\86166\AppData\Local\Programs\Python\Python310\lib\site-packages\scrapy\core\scraper.py:157: UserWarning: The "CrawlScenicSpider.parse" method is a generator and includes a "return" statement with a value different than None. This could lead to unexpected behaviour. Please see https://docs.python.org/3/reference/simple_stmts.html#the-return-statement for details about the semantics of the "return" statement within generators
  warn_on_generator_with_return_value(spider, callback)

2023-04-02 17:06:53.234 | INFO     | scrapy.extensions.logstats:log:48 - Crawled 63 pages (at 63 pages/min), scraped 21 items (at 21 items/min)
2023-04-02 17:07:53.241 | INFO     | scrapy.extensions.logstats:log:48 - Crawled 141 pages (at 78 pages/min), scraped 53 items (at 32 items/min)
2023-04-02 17:08:53.233 | INFO     | scrapy.extensions.logstats:log:48 - Crawled 221 pages (at 80 pages/min), scraped 87 items (at 34 items/min)
